#!./.mnist-keras/bin/python
import json
import os

import docker
import fire



import os
import tensorflow as tf
import threading
import psutil
import pandas as pd

import pickle
import json
import numpy as np
from sklearn import metrics
import yaml

import tensorflow
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Activation
from tensorflow.keras.layers import Dense, BatchNormalization, Flatten, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras import activations
import random
from sklearn.metrics import confusion_matrix,precision_recall_curve
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score
from sklearn.metrics import roc_auc_score, auc
from tensorflow.keras import backend as K


from sklearn.utils import class_weight
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

from fedn.utils.kerashelper import KerasHelper
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback


# def _get_data_path():
#     # Figure out FEDn client number from container name
#     client = docker.from_env()
#     container = client.containers.get(os.environ['HOSTNAME'])
#     number = container.name[-1]
#
#     # Return data path
#     return f"/var/data/clients/{number}/mnist.npz"

SenAtSpe = tf.keras.metrics.SensitivityAtSpecificity(0.5)


class Training_Results:
    def __init__(self, Epoch, Acc, Val_Acc, F1, Val_F1, SenAtSpe, val_SenAtSpe):
        # Epoc No
        self.Epoch = Epoch

        # Validation
        self.Val_Acc = Val_Acc
        self.Val_F1 = Val_F1
        self.Val_SenAtSpe = val_SenAtSpe

        # Training
        self.Acc = Acc
        self.F1 = F1
        self.SenAtSpe = SenAtSpe

    def Update_Results(self, Epoch, Acc, Val_Acc, F1, Val_F1, SenAtSpe, val_SenAtSpe):
        # Epoc No
        self.Epoch = Epoch

        # Validation
        self.Val_Acc = Val_Acc
        self.Val_F1 = Val_F1
        self.Val_SenAtSpe = val_SenAtSpe

        # Training
        self.Acc = Acc
        self.F1 = F1
        self.SenAtSpe = SenAtSpe


class Metrics(Callback):
    def __init__(self, validation):
        super(Metrics, self).__init__()
        self.validation = validation

        print('validation shape', len(self.validation[0]))

    def on_train_begin(self, logs={}):
        self.val_f1s = []
        self.val_recalls = []
        self.val_precisions = []

    def on_epoch_end(self, epoch, logs={}):
        global best_epoch
        Save_Epoc_Path='.'

        val_targ = self.validation[1]
        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()

        val_f1 = f1_score(val_targ, val_predict)
        self.val_f1s.append(round(val_f1, 3))

        #         if(val_f1 >= best_epoch.Val_F1):
        if ((logs['val_sensitivity_at_specificity'] > best_epoch.Val_SenAtSpe) or
                ((logs['val_sensitivity_at_specificity'] == best_epoch.Val_SenAtSpe) and
                 (val_f1 >= best_epoch.Val_F1))):
            best_epoch.Update_Results(epoch, logs['accuracy'], logs['val_accuracy'], val_f1, val_f1,
                                      logs['sensitivity_at_specificity'], logs['val_sensitivity_at_specificity'])

            self.model.save(os.path.join(Save_Epoc_Path, "best_model.h5"))
            print("best Epoch: ", best_epoch.Epoch, " Acc:", round(best_epoch.Val_Acc, 3), " F1: ", best_epoch.Val_F1,
                  " SenAtSpe: ", best_epoch.Val_SenAtSpe)

def _compile_model(features_count, lr):
    # Set input shape
    input_shape = (features_count,)

    # Define model
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=input_shape))
    model.add(tf.keras.layers.Dense(8, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(4, activation='relu'))
    model.add(tf.keras.layers.Dropout(0.5))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
                  metrics=['accuracy', SenAtSpe])
    print(" --------------------------------------- ")
    print(" ------------------Full MODEL CREATED------------------ ")
    print(" --------------------------------------- ")

    return model


def _load_data(data_path='../var/data', is_train=True):
    # Load data
    # if data_path is None:
    #     data = np.load(_get_data_path())
    # else:
    #     data = np.load(data_path)

    if is_train:
        df = pd.read_csv(os.path.join(data_path, 'train.csv'))
        X = df.drop('unplanned_flag', axis=1)
        y = df['unplanned_flag']
    else:
        # Test error (Client has a small dataset set aside for validation)
        df1 = pd.read_csv(os.path.join(data_path, 'test.csv'))
        X = df1.drop('unplanned_flag', axis=1)
        y = df1['unplanned_flag'
        print("X shape: : ", X.shape)


    return X, y


def init_seed(out_path='seed.npz'):
    weights = _compile_model(features_count=59, lr=0.003).get_weights()
    helper = KerasHelper()
    helper.save_model(weights, out_path)


def train(in_model_path, out_model_path, data_path='../../var/data'):
    global best_epoch = Training_Results(0, 0, 0, 0, 0, 0, 0)
    # Load model settings
    with open('settings.yaml', 'r') as fh:
        try:
            settings = dict(yaml.safe_load(fh))
        except yaml.YAMLError as e:
            raise(e)

    # Load data
    x_train, y_train = _load_data(data_path)
    x_test, y_test = _load_data(data_path, is_train=False)



    print("-- RUNNING TRAINING --", flush=True)

    # Load model
    model = _compile_model(features_count=59, lr=0.003)
    helper = KerasHelper()
    weights = helper.load_model(in_model_path)
    model.set_weights(weights)

    es = EarlyStopping(monitor='val_loss', patience=50, mode='min')

    # Fit the model
    #########################
    best_epoch.Update_Results(0, 0, 0, 0, 0, 0, 0)
    weights = class_weight.compute_class_weight('balanced', classes=[0, 1], y=y_train)
    Weight_Dic = {idx: val for idx, val in enumerate(weights)}

    model.fit(x_train, y_train, epochs=200,
                         batch_size=90, verbose=1,
                         shuffle=True, validation_data=(x_test, y_test),
                         class_weight=Weight_Dic,
                         callbacks=[es, Metrics(validation=(x_test, y_test))])


# model.fit(x_train, y_train, batch_size=settings['batch_size'], epochs=settings['epochs'], verbose=True)

    Model_T = tf.keras.models.load_model('best_model.h5')

    # Save
    weights = Model_T.get_weights()
    helper.save_model(weights, out_model_path)


def sensitivity(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    return true_positives / (possible_positives + K.epsilon())


def specificity(y_true, y_pred):
    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))
    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))
    return true_negatives / (possible_negatives + K.epsilon())
def validate(in_model_path, out_json_path, data_path='../../var/data'):
    # Load data
    # x_train, y_train = _load_data(data_path)
    X_test, Y_test = _load_data(data_path, is_train=False)

    print("-- RUNNING VALIDATION --", flush=True)
    # Load model
    model = _compile_model(features_count=59, lr=0.003)
    helper = KerasHelper()
    weights = helper.load_model(in_model_path)
    model.set_weights(weights)

    # Evaluate
    Y_test = Y_test.astype(int)
    Pred_y = model.predict(X_test)

    Pred_y2 = np.ravel((Pred_y > 0.5).astype('int'))

    # matrix = confusion_matrix(Y_test, Pred_y2)
    Acc = accuracy_score(Y_test, Pred_y2)
    F1 = f1_score(Y_test, Pred_y2)
    AUC = roc_auc_score(Y_test, Pred_y2)
    report = classification_report(Y_test, Pred_y2)

    Y_test = Y_test.astype(float)
    Pred_y2 = Pred_y2.astype(float)

    sensitivity_res = sensitivity(Y_test, Pred_y2)
    specificity_res = specificity(Y_test, Pred_y2)

    prc, recal, thetas = precision_recall_curve(Y_test, Pred_y2)
    AUC_PR = auc(recal, prc)


    # model_score_test = model.evaluate(x_test, y_test, verbose=0)
    # print('Test loss:', model_score_test[0])
    # print('Test accuracy:', model_score_test[1])
    # y_pred = model.predict(x_test)
    # y_pred = np.argmax(y_pred, axis=1)
    # clf_report = metrics.classification_report(y_test.argmax(axis=-1), y_pred)
    #
    # print(clf_report)



    # JSON schema
    report = {
        "classification_report": clf_report,
        "accuracy": Acc,
        "AUC": AUC,
        "AUC_PR": AUC_PR,
        "F1": F1,
        "sensitivity": sensitivity_res,
        "specificity": specificity_res,
    }
    print("-- VALIDATION COMPLETE! --", flush=True)
    # Save JSON
    with open(out_json_path, "w") as fh:
        fh.write(json.dumps(report))


if __name__ == '__main__':

    fire.Fire({
        'init_seed': init_seed,
        'train': train,
        'validate': validate,
        # '_get_data_path': _get_data_path,  # for testing
    })
